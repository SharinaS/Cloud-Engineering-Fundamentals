# S3

## Index

* [About S3](#About-S3)
* [Cross Region Replication](#Cross-Region-Replication)
* [Data Consistency](#Data-Consistency)
* [Encryption](#Encryption)
  * [KMS](#KMS)
* [Lifecycle Policies](#Lifecycle-Policies)
* [Multipart Upload](#Multipart-Upload)
* [Notifications](#Notifications)
* [Object Ownership](#Object-Ownership)
* [Projects](#Projects)
* [Size](#Size)
* [Tiered Storage](#Tiered-Storage)
  * [Glacier](#Glacier)

# About S3

[Read the S3 FAQs](https://aws.amazon.com/s3/faqs/)

> Keyword: Durable

S3 is one of the oldest services in AWS.

Amazon S3 is object storage built to store and retrieve any amount of data from anywhere on the Internet. It’s a simple storage service that offers an extremely durable, highly available, and infinitely scalable data storage infrastructure at very low costs.

Simple Storage Service 

* For static files
* Not suitable for installing an OS on
* Not used for hosting a DB

For every account, by default, you can have **100 buckets**.

# Use Cases

Hosting static websites

*  You can host a static website on Amazon Simple Storage Service (Amazon S3). On a static website, individual webpages include static content. They might also contain client-side scripts. To host a static website, you configure an Amazon S3 bucket for website hosting, allow public read access, and then upload your website content to the bucket.

A media store for the CloudFront service

* Amazon S3 is an excellent storage facility for your media assets. It is infinitely scalable, has built-in redundancy, and is available to you on a pay-as-you-go basis. For example, if you want to deliver or stream video files to your global users, all you need to do is to **put your content in an S3 bucket and create a CloudFront distribution** that points to the bucket. Your user’s video player will use CloudFront URLs to request the video file. The request will be directed to the best edge location, based on the user’s location. The Amazon Cloudfront Content Delivery Network (CDN) will serve the video from its cache, fetching it from the S3 bucket if it has not already been cached. The CDN caches content at the edge locations for consistent, low-latency, high-throughput video delivery.

# Object-Based Storage

S3 is Object-based

* It allows you to upload files

Object is made up of:

* Key
  * name of the object
* Value 
  * the data
  * made up of a sequence of bytes
* Version ID 
  * important for versioning
  * S3 allows for versioning!
* Metadata 
  * data about data you're storing
* Subresources
  * Access control lists - the permissions of that object. You can lock each object down individually (versus bucket level)
  * Torrent(s)

Not suitable for installing an operating system on.

# Scales Automatically

Amazon S3 automatically scales to high request rates.

See the [aws whitepapers](https://d1.awsstatic.com/whitepapers/AmazonS3BestPractices.pdf)

# Increase Performance

Other AWS services can also help accelerate performance for different application architectures. For example, if you want higher transfer rates over a single HTTP connection or single-digit millisecond latencies, use **Amazon CloudFront** or **Amazon ElastiCache** for caching with Amazon S3. 

See the [aws whitepapers](https://d1.awsstatic.com/whitepapers/AmazonS3BestPractices.pdf)


# Upload

Upon upload, an HTTP 200 code will come back to your browser is the upload is successful. 

# Object Ownership

Object ownership is important for managing permissions using a bucket policy. For a bucket policy to apply to an object in the bucket, the object must be owned by the account that owns the bucket. 

You can also manage object permissions using the object's ACL. However, object ACLs can be difficult to manage for multiple objects, so it's a best practice to use the bucket policy as a centralized method for setting permissions.

By default, an S3 object is owned by the account that uploaded the object.

## To make the Destination the Owner

Grant the destination account the **permissions to perform a cross-account copy** to make sure the destination owns the copied objects. 

You can also change the ownership of an object by changing its access control list (ACL) to bucket-owner-full-control. However, object ACLs can be difficult to manage for multiple objects, so it's a best practice to grant programmatic cross-account permissions to the destination account.

To be sure that a destination account owns an S3 object copied from another account, grant the destination account the permissions to perform the cross-account copy. Follow these steps to configure cross-account permissions to copy objects from a source bucket in Account A to a destination bucket in Account B:

1. Attach a bucket policy to the source bucket in Account A.
2. Attach an AWS Identity and Access Management (IAM) policy to a user or role in Account B.
3. Use the IAM user or role in Account B to perform the cross-account copy.

# Size

**File size** of an object stored in a bucket can be from 0 Bytes to 5 TB.

**Bucket** size is unlimited - storage is unlimited.

The largest **object** that can be uploaded in a **single PUT** is 5 GB. 

## Multipart Upload

For objects larger than 100 megabytes, customers should consider using the **Multipart Upload** capability.

The Multipart upload API enables you to upload large objects in parts. You can use this API to upload new large objects or make a copy of an existing object. Multipart uploading is a three-step process: you initiate the upload, you upload the object parts, and after you have uploaded all the parts, you complete the multipart upload. Upon receiving the complete multipart upload request, Amazon S3 constructs the object from the uploaded parts and you can then access the object just as you would any other object in your bucket.

# Universal Namespace

S3 is a universal namespace, so the names must be unique globally.

This is because it is creating a web address. It will have the bucket name, region, amazonaws.com. 

* It will create a DNS web name, in other words.

# Data Consistency

## PUTS of new Objects 

*Read after Write Consistency*  

* ... which means as soon as you upload the object, you're able to read it.

Amazon S3 provides read-after-write consistency for PUTS of new objects in your S3 bucket in all regions.

## HEAD or GET requests

*Eventual Consistency*

... with one caveat: if you make a HEAD or GET request to the key name (to find if the object exists) before creating the object, Amazon S3 provides eventual consistency for read-after-write.

## Overwrite PUTS and DELETES

*Eventual Consistency*  

Can take some time to propogate. this means it will take a bit to update or delete a file. Before the change propogates, you'll be reading the *earlier* version. 

## PUT to an Existing Key

Updates to a single key are atomic. For example, if you PUT to an existing key, a subsequent read might return the *old data or the updated data*, but it will never return corrupted or partial data. This usually happens if your application is using **parallel requests on the same object**.

## Parallel Requests

Amazon S3 achieves high availability by replicating data across multiple servers within Amazon's data centers. If a PUT request is successful, your data is safely stored. However, information about the changes must replicate across Amazon S3, which can take some time, and so you might observe the following behaviors:

- A process writes a new object to Amazon S3 and immediately lists keys within its bucket. Until the change is fully propagated, the object might not appear in the list.

- A process replaces an existing object and immediately attempts to read it. Until the change is fully propagated, Amazon S3 might return the prior data.

- A process deletes an existing object and immediately attempts to read it. Until the deletion is fully propagated, Amazon S3 might return the deleted data.

- A process deletes an existing object and immediately lists keys within its bucket. Until the deletion is fully propagated, Amazon S3 might list the deleted object.

Amazon S3’s support for parallel requests means you can scale your S3 performance by the factor of your compute cluster, without making any customizations to your application. 

## Object Locking

Amazon S3 does not currently support Object Locking. If two PUT requests are simultaneously made to the same key, the request with the latest timestamp wins. If this is an issue, you will need to build an object-locking mechanism into your application.

This is different from the Amazon S3 Object Lock feature which prevents an object from being deleted or overwritten for a fixed amount of time or indefinitely.

# S3 Amazon Guarantee

Built for 99.99% availability, and guaranteed at 99.99% availability.

99.999999999999 (eleven 9's) guarantees durability, so if an object is put into S3, it will likely not be lost, in other words. 

# Object Deletion

## MFA Delete

You can turn on multi-factor authentication to delete an object, which means the object is *protected* so it isn't accidentally deleted.

## How an Object Got Deleted

To find out how an S3 object was deleted, you can review either **server access logs** or **AWS CloudTrail logs**.

CloudTrail logs can **track object-level data events** in an S3 bucket, such as GetObject, DeleteObject, and PutObject. You must enable CloudTrail logs for object-level events 

*By default, CloudTrail records bucket-level events*.

Note: Logging must be enabled on the bucket prior to the deletion event. You receive logs only for events that occurred after logging was enabled.

### Server Access Logs

*By default, Amazon Simple Storage Service (Amazon S3) doesn't collect server access logs.* 

When you enable logging, Amazon S3 delivers access logs for a source bucket to a target bucket that you choose. The target bucket must be in the same AWS Region as the source bucket and must not have a default retention period configuration.

Server access logging provides **detailed records for the requests** that are made to an S3 bucket. 

Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill.

An access log record contains details about the requests that are made to a bucket. This information can include the request type, the resources that are specified in the request, and the time and date that the request was processed.

 [See the docs](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/server-access-logging.html).

 #### Use with CloudTrail

 You can use AWS CloudTrail logs together with server access logs for Amazon S3. CloudTrail logs provide you with detailed API tracking for Amazon S3 bucket-level and object-level operations, while server access logs for Amazon S3 provide you visibility into object-level operations on your data in Amazon S3.

# Transfer Acceleration
File transfers occur very fast and securely.

**CloudFront's Edge Locations** are used to make this happen. When a user uploads a file, it uploads to an edge location, versus to the S3 bucket. Then, this goes over the Amazon backbone network, taking the contents directly back to where the bucket is located. This speeds up user's upload times. 

Uses CloudFront Edge Network to speed up S3 uploads. You upload to an S3 edge location, and then that object is transfered over to S3.

You'll get a URL to upload to, which looks something like: "_____.s3-accelerate.amazonaws.com"

There is a tool to test it: "Amazon S3 Transfer Acceleration Speed Comparison"

* Shows multiple bar graphs after running it that shows various regions and how fast it is in that region.

# Fees

* Storage - the more you store, the more you're billed - you pay by the gig
  * If you have a lot of objects, it incurs "Monitoring and Automation," which is per 1000 objects.
* Requests - more requests, the more the money
* Storage Management Pricing, so the tier you choose will determine the price
* Data Transfer Pricing
* Transfer Acceleration
* Cross Region Replication

# Tiered Storage

When in doubt, choose Intelligent Tiering.

## Cost from highest to lowest (6 options)

S3 Standard > S3-IA > S3-Intelligent Tiering > S3 One Zone - IA > S3 Glacier > S3 Glacier Deep Archive. 

## Storage Classes

All the following have a first byte latency (how quickly you can access your data) of milliseconds.

### S3 Standard

* 99.99% availability
* 99.99999999999% durability
* stored redundently in multiple facilities
* Can handle the loss of 2 facilities at the same time
* Most expensive, so it's good to avoid, unless you've got 1000s or millions of objects. Consider using Intelligent Tiering instead. 

### S3 - IA

* IA = infrequently accessed 
* OneZone IA is fine for situtations when users 'post' a ton of material, and an organization only needs to access it to find relevant material.
* For data that does not need to be accessed all that often
* Less expensive than S3 Standard.
* For when you want the same durability and availability as S3 Standard, and you need rapid access when required... but you want to pay a lower fee.
* You are charged a retrieval fee.
* See section on [constraints](#Constraints)

### S3 One Zone - IA

* Storage in only *one availability zone*
* For infrequently accessed data
* Takes the place of RRS, a service that is being phased out. RRS is like S3 One Zone, when you want something cheap and you don't care about accessing the data much.
* A good option if you don't want to use Intelligent Tiering / Standard.
* Good to choose if you don't care about redundancy, and want something cheaper than S3 Standard, and still want to frequently access the object.
* Danger - if you lose the zone, you'll lose your data - It's for data you can easily reproduce.
* See section on [constraints](#Constraints)

### S3 - Intellgient Tiering

* AI will analyze how often an object is accessed, and will automatically move data to the most cost-effective tier.
* No performance impact or operational overhead.
* A good tier to make use of.

## Glacier

For data archiving, ie, having to save data for 7 years legally.

### Definition of **vault**

Vaults are the containers for storing the archives.

(meanwhile, S3 Glacier archives data.) 

## Types of Glacier

### S3 Glacier

* Costs the same or less than on-premise solutions
* Retrieval times configurable from minutes to hours (first byte latency)

### S3 Glacier Deep Archive

* Lowest cost of all storage classes.
* Retrieval time is *12 hours* (so you won't get the data for half a day after submitting a request, which is its first byte latency)

## Restoring An Object From Glacier

Note that restoring an object from Glacier costs money.

From A Cloud Guru: "Cost of retrieval of information from Glacier can go up dependent on how quickly you require the data and how much data is to be retrieved. 

### Expedited Retrieval

Expedited retrievals allow you to quickly access your data stored in the S3 Glacier storage class when occasional urgent requests for a subset of archives are required, but at the highest cost. 

### Standard Retrieval

Standard retrievals allow you to access any of your archived objects within several hours, this is faster than bulk (averaging around 12 hours) but more expensive. 

### Bulk Retrieval

Bulk retrievals are the lowest-cost retrieval option in Amazon S3 Glacier, enabling you to retrieve large amounts, even petabytes, of data inexpensively"

### The least expensive way of restoring an object

--> bulk retrieval <-- 

* You'll have to note how many days you want the object restored for.

# Lifecycle Policies

Automates moving your objects between the different storage tiers

You can specify that when an object is x days old, move that object to another bucket or glacier.

Can be used with Versioning

Can be applied to current version and previous versions.

## Constraints

### Size of Objects

For larger objects, there is a cost benefit for transitioning to STANDARD_IA or ONEZONE_IA. 

Amazon S3 does not transition objects that are smaller than 128 KB to the STANDARD_IA or ONEZONE_IA storage classes because it's not cost effective.

### Time

Objects must be stored at least 30 days in the current storage class before you can transition them to STANDARD_IA or ONEZONE_IA. 

For example, you cannot create a lifecycle rule to transition objects to the STANDARD_IA storage class one day after you create them. Amazon S3 doesn't transition objects within the first 30 days because newer objects are often accessed more frequently or deleted sooner than is suitable for STANDARD_IA or ONEZONE_IA storage.

This limitation does not apply on INTELLIGENT_TIERING, GLACIER, and DEEP_ARCHIVE storage class.

### Current

If you are transitioning noncurrent objects (in versioned buckets), you can transition only objects that are at least 30 days noncurrent to STANDARD_IA or ONEZONE_IA storage.

# Security - Bucket Permissions and Management

By default, all newly created buckets are private.

Set up access control with:

* Bucket policies
* Access control lists

Allows for securing of data

## Access Logs

S3 buckets can be configured to **create access logs**, which keeps track of requests made to the bucket. 

You can have those access logs sent to another bucket in the same or another account for storage.

## Bucket Permissions

### Bucket Policies

Lets you choose whether the bucket is blocked to all public access, which is the default, or open it up to various levels of public access. 

### Access Control List 

ACL = Access Control List

Allows you to *set detailed permissions* all the way down to individual objects. 

If there's a particular object in a particular bucket, you can block down to that object, ie a spreadsheet, that no one is allowed to view it except for Benedict Cumberbatch, who works in the entertainment department.

Can use a bucket ACL or a bucket policy to control permissions.

## Secure Access to Private Files in S3

Three ways to gain access to files stored in S3: 

### Signed URLs

### Signed Cookies

* Signed URLs and Signed Cookies are different ways to ensure that users attempting access to files in an S3 bucket can be authorised. 
  * One method generates URLs and the other generates special cookies but they both require the creation of an application and policy to generate and control these items. 

### Origin Access Identity 

a virtual user identity used to give the CloudFront distribution permission to fetch a private object from an S3 bucket."

# Encryption 

S3 has encryption in transit and at rest. There is also client side.

## Encryption in Transit

Encryption in Transit is achieved with:

* HTTPS
* SSL/TLS

If you have HTTPS in the browswer, the traffic is encrypted.

### Encryption in Transit, regarding S3

In the S3 Console, as long as you're using HTTPS, all the files you upload will be encrypted. 

## Encryption at Rest 

Achieved by both:

* server side encryption
* client side encryption

### Server Side 

Server-side encryption is the encryption of data at its destination by the application or service that receives it. Server-side encryption is about data encryption at rest—that is, Amazon S3 **encrypts your data at the object level** as it writes it to disks in its data centers and decrypts it for you when you access it. 

As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects. For example, if you share your objects using a pre-signed URL, that URL works the same way for both encrypted and unencrypted objects.

You have three mutually exclusive options depending on how you choose to manage the encryption keys:

* Use Server-Side Encryption with Amazon **S3-Managed** Keys (SSE-S3) -- S3 manages all our encryption for us

* Use Server-Side Encryption with AWS **KMS-Managed** Keys (SSE-KMS) --Keys are supplied from the KMS service

* Use Server-Side Encryption with **Customer-Provided** Keys (SSE-C) -- you manage the encryption

#### SSE-S3

Each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.

#### SSE-KMS

[Check out my notes on KMS](https://github.com/SharinaS/Cloud-Engineering-Fundamentals/blob/master/kms.md).

Similar to SSE-S3, but with some additional benefits and charges for using this service. There are separate permissions for the use of a CMK that provides added protection against unauthorized access of your objects in Amazon S3. SSE-KMS also provides you with an audit trail that shows when your CMK was used and by whom. Additionally, you can create and manage customer-managed CMKs or use AWS managed CMKs that are unique to you, your service, and your Region.

#### SSE-C

You manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects.

### Client Side 

You encrypt the object and upload it to S3.

Client-side encryption is the act of encrypting data before sending it to Amazon S3. You manage the encryption process, the encryption keys, and related tools.

To enable client-side encryption, you have the following options:

* Use an AWS KMS-managed customer master key.
* Use a client-side master key.

#### KMS-managed customer key

When using an AWS KMS-managed customer master key to enable client-side data encryption, you provide an AWS KMS customer master key ID (CMK ID) to AWS. 

#### Client-side master key

On the other hand, when you use client-side master key for client-side data encryption, your client-side master keys and your unencrypted data are never sent to AWS. 

It's important that you safely manage your encryption keys because if you lose them, you can't decrypt your data.

##### Upload

When uploading an object - You provide a client-side master key to the Amazon S3 encryption client. The client uses the master key only to encrypt the data encryption key that it generates randomly. The process works like this:

1. The Amazon S3 encryption client generates a one-time-use symmetric key (also known as a **data encryption key** or **data key**) locally. It uses the data key to encrypt the data of a single Amazon S3 object. *The client generates a separate data key for each object.*

2. The client encrypts the data encryption key using the master key that you provide. The client uploads the **encrypted data key** and its **material description** as part of the object metadata. *The client uses the material description to determine which client-side master key to use for decryption.*

3. The client uploads the encrypted data to Amazon S3 and saves the encrypted data key as object metadata (x-amz-meta-x-amz-key) in Amazon S3.

##### Download

When downloading an object - The client downloads the encrypted object from Amazon S3. *Using the material description from the object's metadata, the client determines which master key to use* to decrypt the data key. 

The client uses that master key to (1) decrypt the data key and then (2) uses the data key to decrypt the object.

### How to Change Encryption in the Console

* Go to the bucket, click on the object's radio button. 
* On the right, a blue titled box will appear. The section called "Properties" has a row called "Encryption."
* You can choose None, AES-256 or AWS-KMS
  * AES-256 is the S3 Managed Key
  * AWS-KMS allows you to use aws/s3 keys
    * or you can choose to use your own custom keys (custom KMS ARN).

![screenshot of encrypted object](/assets/encryption.png)

# Share S3 Buckets Across Accounts

There are 3 ways:

* Use Bucket Policies and IAM
  * applies across the entire bucket b/c it's a bucket policy
  * programmatic access only
* Use Bucket [ACL](#Access-Control-List) and IAM
  * on individual objects b/c we're using ACLs 
  * programmatic access only
* Use cross-account IAM roles
  * programmatic *and* console access
  * allows for being in one account, then assume a role and then go into another AWS account and do something (like create a bucket)

# Versioning

Once enabled, you can't disable it, only suspended.

Integrates with lifecycle rules

> Uses MFA, which prevents folks from deleting objects. 

## How To Add Versioning

* Create a bucket.
* Change the settings for the bucket so objects in the bucket can be public (this will make experimenting with the objects easier, since you'll be able to see their content).
* Go to properties and click on Versioning. Choose `Enable versioning`. 
* Upload your first object, say a txt file with a little content, and make it public.
* Edit your txt file and re-upload. 
* Up above the object listed in the bucket, you'll see an option to `Hide` or `Show` Versions. Toggle it to say `Show`. You should now see two versions of the same object! 

![screenshot of S3 version option](/assets/s3versions.png)

* To view the newest version, you'll have to make it public, FYI. 

## Storage Consideration (= billing consideration)
When you are uploading new versions after editing files, note that each new version can be much larger than the last one. When architecting, keep this in mind, since if you expect to make many changes, you may find it's better to not upload every version, since storage requirements can exponentially increase within the bucket. 

## Deleting After Adding Versioning

To delete a version, the user must have MFA set-up, which provides an additional layer of security against accidental deletion.

To delete a particular version, click on that the radio button next to that version. To do this, make sure that the Versions toggle is set to `Show`.

When deleting the entire object, and you have Versions toggled to `Hide`, it will create a new version of that object that is of status deleted. You can un-delete the object by deleting that version, and it will restore that object to the prior version.

# Amazon Select

An Amazon S3 feature that makes it easy to retrieve specific data from the contents of an object using simple **SQL expressions** without having to retrieve the entire object.

# Lifecycle Management

## About

When you add a lifecycle rule, it will automate transitioning your objects to different tiers of storage. When you specify it, it will take care of expiring the objects too. 

## How To

Go to S3. Go into a bucket that has versioning enabled. Click on the tab called "Management." You can then click on the button called "Lifecycle." 

Click the button "Add lifecycle rule," which will add a rule to the bucket.

Can add tags when adding a rule name. Tags will make sure the rule only applies to those tags. If you want to apply the rule to the entire bucket, skip using tags.

Next, you specify whether to have the rule apply to current or past versions of the object in the bucket. 

* When you click on "Current version," you need to `+Add transition`. This will let you choose what tier to transition to after a certain number of time. 

![screenshot of lifecycle rules](/assets/lifecyclerules.png)

You can then "Configure expiration" for both current and previous versions. When you click on the radio button for "Clean up incomplete multipart uploads, this refers to uploads that occur in various pieces. Sometimes you can have files that don't upload correctly, so this will help clean those poorly loaded files from lingering. 

# Cross Region Replication

A way of replicating objects *across regions* (or in the same region).

**Versioning** must be enabled on both the source and destination buckets for it to work.

When you upload your data in S3, your objects are redundantly stored on multiple devices across multiple facilities within the region only, where you created the bucket. Thus, if there is an outage on the entire region, your S3 bucket will be unavailable if you do not enable Cross-Region Replication, which should make your data available to another region.

Things that won't be replicated:

* Files in *existing buckets* 
  * All subsequent updated files will be replicated automatically though
* Delete markers 
  * If you delete an object in one bucket, it won't be deleted in the other bucket.
* Deleted individual versions or delete markers
  * If you delete a version, it won't be deleted in the other bucket.

If you have a bucket in region 1, and you want to replicate the bucket in region 2 (for disaster recovery and high availability). Upon upload in region 1, and you have cross region replication turned on, the bucket in region 2 will appear.

# Notifications

The Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration identifying the events you want Amazon S3 to publish, and the destinations where you want Amazon S3 to send the event notifications.

## Destinations

Amazon S3 supports the following destinations where it can publish events:

* SNS
* SQS
* Lambda

Amazon Simple Notification Service (Amazon SNS) topic - A web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients.

Amazon Simple Queue Service (Amazon SQS) queue - Offers reliable and scalable hosted queues for storing messages as they travel between computer.

AWS Lambda - AWS Lambda is a compute service where you can upload your code and the service can run the code on your behalf using the AWS infrastructure. You package up and upload your custom code to AWS Lambda when you create a Lambda function

## How To Use Notifications

Here’s what you need to do in order to start using this new feature with your application:

Create the queue, topic, or Lambda function (which I’ll call the target for brevity) if necessary.

Grant S3 permission to publish to the target or invoke the Lambda function. For SNS or SQS, you do this by applying an appropriate policy to the topic or the queue. For Lambda, you must create and supply an IAM role, then associate it with the Lambda function.

Arrange for your application to be invoked in response to activity on the target. As you will see in a moment, you have several options here.

Set the bucket’s Notification Configuration to point to the target.

[See the AWS Developer Guide](https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html).

# Make a Bucket in S3 Using the Terminal
Make sure you have access to your EC2 instance via the terminal (for setting up an EC2 instance, [see my EC2.md file](https://github.com/SharinaS/Cloud-Engineering-Fundamentals/blob/master/EC2.md) - you'll be in a directory that looks like:  

```
[root@ip-111-11-11-11 ec2-user]#
```
### Make a bucket
The format is **aws console + the service + action + name of new thing**.

*Note that the name of the bucket must be lowercase.*
```
aws s3 mb s3://supercooldemobucket
```

### List all the buckets you have in S3:
```
aws s3 ls
```

### Upload Content
To upload content into the bucket from the current location:
(for example, you can make a demo txt file with some content in it using `echo "this is some cool content" > cool-file-name.txt`

... use `ls` to prove to yourself that the file now exists within the current directory)

```
aws s3 cp demoCCP_textfile.txt s3://supercooldemobucket
```
![screenshot of bucket in amazon console](/assets/demoTextFile.png)

-----------
-----------

# Host a Website - Static - on S3

Can use a Route53 domain (or subdomain) name. 

Create a new bucket. Name it to match your domain name, with the .com at the end. 

```
hello.com
```

### Open the bucket to the world, so you can host a website

Click the radio button next to the bucket.

Above, click "Edit Public Access Settings"

* They should all be unchecked.
* Click "Save"

Your bucket should now say, under "Access," "Objects can be public"

Go into the bucket - click on the URL

Click on the "Properties" tab.

Click on the box that says "Static website hosting"

* Click on "Use this bucket to host a website"
* Type in "index.html" under Index document and "error.html" under Error document (make sure to add a custom one, if you like). 
* Hit "save"

Back in the bucket, click the tab that says "Upload Files"

* Add your index and error files

Make those files public up in the Actions pulldown.

# Projects

My [full-stack Java app - Music Central](https://github.com/SharinaS/music-central) - makes use of S3. The readme has notes from my experience of integrating S3 into the app.

# Resources
* A Cloud Guru's [AWS Certified Cloud Practitioner](https://acloud.guru/learn/aws-certified-cloud-practitioner) Course
* A Cloud Guru's [AWS Certified Solutions Architect Associate](https://acloud.guru) Course
* [S3 FAQs](https://aws.amazon.com/s3/faqs/)
* [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/)
