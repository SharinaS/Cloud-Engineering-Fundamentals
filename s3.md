# S3

## Projects

My [full-stack Java app - Music Central](https://github.com/SharinaS/music-central) - makes use of S3. The readme has notes from my experience of integrating S3 into the app.


# About S3

[Read the S3 FAQs for the exam](https://aws.amazon.com/s3/faqs/)

S3 is one of the oldest services in AWS.

Simple Storage Service 

* For static files
* Not suitable for installing an OS on
* Not used for hosting a DB

S3 is a safe place to store files 

For every account, by default, you can have **100 buckets**. 

# Object-Based Storage

S3 is Object-based

* It allows you to upload files

Object is made up of:

* Key
  * name of the object
* Value 
  * the data
  * made up of a sequence of bytes
* Version ID 
  * important for versioning
  * S3 allows for versioning!
* Metadata 
  * data about data you're storing
* Subresources
  * Access control lists - the permissions of that object. You can lock each object down individually (versus bucket level)
  * Torrent(s)

Not suitable for installing an operating system on.

# Upload

Upon upload, an HTTP 200 code will come back to your browser is the upload is successful. 

# Size

**File size** of an object stored ina a bucket can be from 0 Bytes to 5 TB.

**Bucket** size is unlimited - storage is unlimited.

# Universal Namespace

S3 is a universal namespace, so the names must be unique globally.

This is because it is creating a web address. It will have the bucket name, region, amazonaws.com. 

* It will create a DNS web name, in other words.

# Data Consistency

## PUTS of new Objects 

S3 has *Read after Write* Consistency  

* ... which means as soon as you upload the object, you're able to read it.

### S3 Can Accomodate much higher PUT and GET levels than in the past

Until 2018 there was a hard limit on S3 puts of 100 PUTs per second. To achieve this care needed to be taken with the structure of the name Key to ensure parallel processing. As of July 2018 the limit was raised to 3500 and the need for the Key design was basically eliminated. - A Cloud Guru

## Overwrite PUTS and DELETES

S3 has *Eventual Consistency*  

* can take some time to propogates)
* this means it will take a bit to update or delete a file. 
* Before the change propogates, you'll be reading the *earlier* version. 

# S3 Amazon Guarantee

Built for 99.99% availability, and guaranteed at 99.99% availability.

99.999999999999 (eleven 9's) guarantees durability, so if an object is put into S3, it will likely not be lost, in other words. 

# Lifecycle Policies

Automates moving your objects between the different storage tiers

You can specify that when an object is x days old, move that object to another bucket or glacier.

Can be used with Versioning

Can be applied to current version and previous versions.

# MFA Delete

You can turn on multi-factor authentication to delete an object, which means the object is *protected* so it isn't accidentally deleted.

# Transfer Acceleration
File transfers occur very fast and securely.

**CloudFront's Edge Locations** are used to make this happen. When a user uploads a file, it uploads to an edge location, versus to the S3 bucket. Then, this goes over the Amazon backbone network, taking the contents directly back to where the bucket is located. This speeds up user's upload times. 

Uses CloudFront Edge Network to speed up S3 uploads. You upload to an S3 edge location, and then that object is transfered over to S3.

You'll get a URL to upload to, which looks something like: "_____.s3-accelerate.amazonaws.com"

There is a tool to test it: "Amazon S3 Transfer Acceleration Speed Comparison"

* Shows multiple bar graphs after running it that shows various regions and how fast it is in that region.

# Fees

* Storage - the more you store, the more you're billed - you pay by the gig
  * If you have a lot of objects, it incurs "Monitoring and Automation," which is per 1000 objects.
* Requests - more requests, the more the money
* Storage Management Pricing, so the tier you choose will determine the price
* Data Transfer Pricing
* Transfer Acceleration
* Cross Region Replication

# Tiered Storage

When in doubt, choose Intelligent Tiering.

## Cost from highest to lowest (6 options)

S3 Standard > S3-IA > S3-Intelligent Tiering > S3 One Zone - IA > S3 Glacier > S3 Glacier Deep Archive. 

## Storage Classes

All the following have a first byte latency (how quickly you can access your data) of milliseconds.

S3 Standard

* 99.99% availability
* 99.99999999999% durability
* stored redundently in multiple facilities
* Can handle the loss of 2 facilities at the same time
* Most expensive, so it's good to avoid, unless you've got 1000s or millions of objects. Consider using Intelligent Tiering instead. 

S3 - IA

* IA = infrequently accessed 
  * OneZone IA is fine for situtations when users 'post' a ton of material, and an organization only needs to access it to find relevant material.
* For data that does not need to be accessed all that often
* Less expensive than S3 Standard.
* For when you want the same durability and availability as S3 Standard, and you need rapid access when required... but you want to pay a lower fee.
* You are charged a retrieval fee.

S3 One Zone - IA

* Storage in only *one availability zone*
* For infrequently accessed data
* Takes the place of RRS, a service that is being phased out. RRS is like S3 One Zone, when you want something cheap and you don't care about accessing the data much.
* A good option if you don't want to use Intelligent Tiering / Standard.
* Good to choose if you don't care about redundancy, and want something cheaper than S3 Standard, and still want to frequently access the object.
* Danger - if you lose the zone, you'll lose your data.
  * It's for data you can easily reproduce.

S3 - Intellgient Tiering

* AI will analyze how often an object is accessed, and will automatically move data to the most cost-effective tier.
* No performance impact or operational overhead.
* A good tier to make use of.

## Glacier

For data archiving, ie, having to save data for 7 years legally.

### Definition of **vault**

Vaults are the containers for storing the archives.

(meanwhile, S3 Glacier archives data.) 

## Types of Glacier

### S3 Glacier

* Costs the same or less than on-premise solutions
* Retrieval times configurable from minutes to hours (first byte latency)

### S3 Glacier Deep Archive

* Lowest cost of all storage classes.
* Retrieval time is *12 hours* (so you won't get the data for half a day after submitting a request, which is its first byte latency)

## Restoring An Object From Glacier

Note that restoring an object from Glacier costs money.

From A Cloud Guru: "Cost of retrieval of information from Glacier can go up dependent on how quickly you require the data and how much data is to be retrieved. Expedited retrievals allow you to quickly access your data stored in the S3 Glacier storage class when occasional urgent requests for a subset of archives are required, but at the highest cost. Standard retrievals allow you to access any of your archived objects within several hours, this is faster than bulk (averaging around 12 hours) but more expensive. Bulk retrievals are the lowest-cost retrieval option in Amazon S3 Glacier, enabling you to retrieve large amounts, even petabytes, of data inexpensively"

### The least expensive way of restoring an object

--> bulk retrieval <-- 

* You'll have to note how many days you want the object restored for.


# Security - Bucket Permissions and Management

By default, all newly created buckets are private.

Set up access control with:

* Bucket policies
* Access control lists

Allows for securing of data

## Access Logs

S3 buckets can be configured to **create access logs**, which keeps track of requests made to the bucket. 

You can have those access logs sent to another bucket in the same or another account for storage.

## Bucket Permissions

### Bucket Policies

Lets you choose whether the bucket is blocked to all public access, which is the default, or open it up to various levels of public access. 

### Access Control List 

ACL = Access Control List

Allows you to *set detailed permissions* all the way down to individual objects. 

If there's a particular object in a particular bucket, you can block down to that object, ie a spreadsheet, that no one is allowed to view it except for Benedict Cumberbatch, who works in the entertainment department.

Can use a bucket ACL or a bucket policy to control permissions.

## Secure Access to Private Files in S3

From A Cloud Guru:

"There are three options in the question which can be used to secure access to files stored in S3 and therefore can be considered correct. 

* Signed URLs and Signed Cookies are different ways to ensure that users attempting access to files in an S3 bucket can be authorised. 
  * One method generates URLs and the other generates special cookies but they both require the creation of an application and policy to generate and control these items. 
* An Origin Access Identity on the other hand, is a virtual user identity that is used to give the CloudFront distribution permission to fetch a private object from an S3 bucket."

# Encryption 

S3 has encryption in transit and at rest. There is also client side.

## Encryption in Transit

Encryption in Transit is achieved with:

* HTTPS
* SSL/TLS

If you have HTTPS in the browswer, the traffic is encrypted.

### Encryption in Transit, regarding S3

In the S3 Console, as long as you're using HTTPS, all the files you upload will be encrypted. 

## Encryption at Rest 

Achieved by both:

* server side encryption
* client side encryption

### Server Side 

Server side encryption is managed 3 different ways:

* S3 Managed Keys
  * aka **SSE-S3** 
  * S3 manages all our encryption for us 
* AWS Key Management Service
  * aka **Managed Keys SSE-KMS** 
  * Keys are supplied from the KMS service
* Server side encryption with customer provided keys
  * **SSE-C**
  * Customer provides the keys and you manage the encryption

## Client Side 

You encrypt the object and upload it to S3.

Client-side encryption is the act of encrypting data before sending it to Amazon S3. To enable client-side encryption, you have the following options:

* Use an AWS KMS-managed customer master key.
* Use a client-side master key.

When using an AWS KMS-managed customer master key to enable client-side data encryption, you provide an AWS KMS customer master key ID (CMK ID) to AWS. On the other hand, when you use client-side master key for client-side data encryption, your client-side master keys and your unencrypted data are never sent to AWS. It's important that you safely manage your encryption keys because if you lose them, you can't decrypt your data.

### Uploading and Downloading 

how client-side encryption using client-side master key works: 

When uploading an object - You provide a client-side master key to the Amazon S3 encryption client. The client uses the master key only to encrypt the data encryption key that it generates randomly. The process works like this:

1. The Amazon S3 encryption client generates a one-time-use symmetric key (also known as a data encryption key or data key) locally. It uses the data key to encrypt the data of a single Amazon S3 object. The client generates a separate data key for each object.

2. The client encrypts the data encryption key using the master key that you provide. The client uploads the encrypted data key and its material description as part of the object metadata. The client uses the material description to determine which client-side master key to use for decryption.

3. The client uploads the encrypted data to Amazon S3 and saves the encrypted data key as object metadata (x-amz-meta-x-amz-key) in Amazon S3.



When downloading an object - The client downloads the encrypted object from Amazon S3. Using the material description from the object's metadata, the client determines which master key to use to decrypt the data key. The client uses that master key to decrypt the data key and then uses the data key to decrypt the object.

## How to Change Encryption in the Console

* Go to the bucket, click on the object's radio button. 
* On the right, a blue titled box will appear. The section called "Properties" has a row called "Encryption."
* You can choose None, AES-256 or AWS-KMS
  * AES-256 is the S3 Managed Key
  * AWS-KMS allows you to use aws/s3 keys
    * or you can choose to use your own custom keys (custom KMS ARN).

![screenshot of encrypted object](/assets/encryption.png)

# Share S3 Buckets Across Accounts

There are 3 ways:

* Use Bucket Policies and IAM
  * applies across the entire bucket b/c it's a bucket policy
  * programmatic access only
* Use Bucket [ACL](#Access-Control-List) and IAM
  * on individual objects b/c we're using ACLs 
  * programmatic access only
* Use cross-account IAM roles
  * programmatic *and* console access
  * allows for being in one account, then assume a role and then go into another AWS account and do something (like create a bucket)

# Versioning

Once enabled, you can't disable it, only suspended.

Integrates with lifecycle rules

> Uses MFA, which prevents folks from deleting objects. 

## How To Add Versioning

* Create a bucket.
* Change the settings for the bucket so objects in the bucket can be public (this will make experimenting with the objects easier, since you'll be able to see their content).
* Go to properties and click on Versioning. Choose `Enable versioning`. 
* Upload your first object, say a txt file with a little content, and make it public.
* Edit your txt file and re-upload. 
* Up above the object listed in the bucket, you'll see an option to `Hide` or `Show` Versions. Toggle it to say `Show`. You should now see two versions of the same object! 

![screenshot of S3 version option](/assets/s3versions.png)

* To view the newest version, you'll have to make it public, FYI. 

## Storage Consideration (= billing consideration)
When you are uploading new versions after editing files, note that each new version can be much larger than the last one. When architecting, keep this in mind, since if you expect to make many changes, you may find it's better to not upload every version, since storage requirements can exponentially increase within the bucket. 

## Deleting After Adding Versioning
To delete a version, the user must have MFA set-up, which provides an additional layer of security against accidental deletion.

To delete a particular version, click on that the radio button next to that version. To do this, make sure that the Versions toggle is set to `Show`.

When deleting the entire object, and you have Versions toggled to `Hide`, it will create a new version of that object that is of status deleted. You can un-delete the object by deleting that version, and it will restore that object to the prior version.

# Amazon Select

an Amazon S3 feature that makes it easy to retrieve specific data from the contents of an object using simple SQL expressions without having to retrieve the entire object.

# Lifecycle Management

### About

When you add a lifecycle rule, it will automate transitioning your objects to different tiers of storage. When you specify it, it will take care of expiring the objects too. 

### How To

Go to S3. Go into a bucket that has versioning enabled. Click on the tab called "Management." You can then click on the button called "Lifecycle." 

Click the button "Add lifecycle rule," which will add a rule to the bucket.

Can add tags when adding a rule name. Tags will make sure the rule only applies to those tags. If you want to apply the rule to the entire bucket, skip using tags.

Next, you specify whether to have the rule apply to current or past versions of the object in the bucket. 
* When you click on "Current version," you need to `+Add transition`. This will let you choose what tier to transition to after a certain number of time. 

![screenshot of lifecycle rules](/assets/lifecyclerules.png)

You can then "Configure expiration" for both current and previous versions. When you click on the radio button for "Clean up incomplete multipart uploads, this refers to uploads that occur in various pieces. Sometimes you can have files that don't upload correctly, so this will help clean those poorly loaded files from lingering. 

# Cross Region Replication

A way of replicating objects *across regions* (or in the same region).

**Versioning** must be enabled on both the source and destination buckets for it to work.

Things that won't be replicated:

* Files in *existing buckets* 
  * All subsequent updated files will be replicated automatically though
* Delete markers 
  * If you delete an object in one bucket, it won't be deleted in the other bucket.
* Deleted individual versions or delete markers
  * If you delete a version, it won't be deleted in the other bucket.

If you have a bucket in region 1, and you want to replicate the bucket in region 2 (for disaster recovery and high availability). Upon upload in region 1, and you have cross region replication turned on, the bucket in region 2 will appear.

-------------
-------------

# Make a Bucket in S3 Using the Terminal
Make sure you have access to your EC2 instance via the terminal (for setting up an EC2 instance, [see my EC2.md file](https://github.com/SharinaS/Cloud-Engineering-Fundamentals/blob/master/EC2.md) - you'll be in a directory that looks like:  

```
[root@ip-111-11-11-11 ec2-user]#
```
### Make a bucket
The format is **aws console + the service + action + name of new thing**.

*Note that the name of the bucket must be lowercase.*
```
aws s3 mb s3://supercooldemobucket
```

### List all the buckets you have in S3:
```
aws s3 ls
```

### Upload Content
To upload content into the bucket from the current location:
(for example, you can make a demo txt file with some content in it using `echo "this is some cool content" > cool-file-name.txt`

... use `ls` to prove to yourself that the file now exists within the current directory)

```
aws s3 cp demoCCP_textfile.txt s3://supercooldemobucket
```
![screenshot of bucket in amazon console](/assets/demoTextFile.png)

-----------
-----------

# Host a Website - Static - on S3

Can use a Route53 domain (or subdomain) name. 

Create a new bucket. Name it to match your domain name, with the .com at the end. 

```
hello.com
```

### Open the bucket to the world, so you can host a website

Click the radio button next to the bucket.

Above, click "Edit Public Access Settings"

* They should all be unchecked.
* Click "Save"

Your bucket should now say, under "Access," "Objects can be public"

Go into the bucket - click on the URL

Click on the "Properties" tab.

Click on the box that says "Static website hosting"

* Click on "Use this bucket to host a website"
* Type in "index.html" under Index document and "error.html" under Error document (make sure to add a custom one, if you like). 
* Hit "save"

Back in the bucket, click the tab that says "Upload Files"

* Add your index and error files

Make those files public up in the Actions pulldown.

# Resources
* A Cloud Guru's [AWS Certified Cloud Practitioner](https://acloud.guru/learn/aws-certified-cloud-practitioner) Course
* A Cloud Guru's [AWS Certified Solutions Architect Associate](https://acloud.guru) Course
* [S3 FAQs](https://aws.amazon.com/s3/faqs/)
* [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/)
