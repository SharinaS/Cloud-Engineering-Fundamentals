# About S3
S3 is one of the oldest services in AWS. 

Simple Storage Service 
* Static files
* Not suitable for installing an OS on
* Not used for hosting a DB

S3 is a safe place to store files - object files / static files

Upon upload, an HTTP 200 code will come back to your browser is the upload is successful. 


### Object-Based Storage 
Object is made up of:
* Key - name of the object
* Value - the data and is made up of a sequence of bytes
* Version ID - important for versioning
  * S3 allows for versioning!
* Metadata - data about data you're storing
* Subresources
  * Access control lists - the permissions of that object. You can lock each object down individually (versus bucket level)
  * Torrent

### File Size
File size can be from 0 Bytes to 5 TB. 

### Universal Namespace
A universal namespace, so the names must be unique globally. This is because it is creating a web address. It will have the bucket name, region, amazonaws.com. It will create a DNS web name, in other words.

### Data Consistency
S3 has Read after Write consistency for PUTS of new Objects, which means as soon as you upload the object, you're able to read it.

Eventual consistency for overwrite PUTS and DELETES (can take some time to propogates), meaning it will take a bit to update or delete a file. Before the change propogates, you'll be reading the earlier version. 

#### Amazon Guarantees
Built for 99.99% availability, and guaranteed at 99.99% availability.

99.999999999999 (eleven 9's) guarantees durability, so if an object is put into S3, it will likely not be lost, in other words. 

### Lifecycle Management
You can specify that when an object is x days old, move that object to another bucket or glacier.

### Versions
You can have multiple versions within an S3 bucket

### Encrytpion
You can encyrpt the object at rest

### MFA Delete
You can turn on multi-factor authentication to delete an object, which means the object is protected so it isn't accidentally deleted.

### Access Control Lists & Bucket Policies
Allows for securing of data

## S3 Transfer Acceleration
File transfers occur very fast and securely.

CloudFront's Edge Locations are used to make this happen. When a user uploads a file, it uploads to an edge location, versus to the S3 bucket. Then, this goes over the Amazon backbone network, taking the contents directly back to where the bucket is located. This speeds up user's upload times. 


## Cross Region Replication
If you have a bucket in region 1, and you want to replicate the bucket in region 2 (for disaster recovery and high availability). Upon upload in region 1, and you have cross region replication turned on, the bucket in region 2 will appear. 

## Fees
* Storage - the more you store, the more you're billed - you pay by the gig
  * If you have a lot of objects, it incurs "Monitoring and Automation," which is per 1000 objects.
* Requests - more requests, the more the money
* Storage Management Pricing, so the tier you choose will determine the price
* Data Transfer Pricing
* Transfer Acceleration
* Cross Region Replication

## Tiered Storage
When in doubt, choose Intelligent Tiering.

### Storage Classes
All the following have a first byte latency (how quickly you can access your data) of milliseconds.

S3 Standard
* 99.99% availability and 99.99999999999% durability
* stored redundently in multiple facilities
* Can handle the loss of 2 facilities at the same time
* Most expensive, so it's good to avoid, unless you've got 1000s or millions of objects. Consider using Intelligent Tiering instead. 

S3 - IA
* IA = infrequently accessed 
* For data that does not need to be accessed all that often
* Less expensive than S3 Standard.

S3 One Zone - IA
* Storage in only one availability zone
* Infrequently accessed
* Takes the place of RRS, a service that is being phased out. RRS is like S3 One Zone, when you want something cheap and you don't care about accessing the data much.
* A good option if you don't want to use Intelligent Tiering / Standard.
* Good to choose if you don't care about redundancy, and want something cheaper than S3 Standard, and still want to frequently access the object.

S3 - Intellgient Tiering
* AI will analyze how often an object is accessed, and will automatically move data to the most cost-effective tier.
* No performance impact or operational overhead.
* A good tier to make use of.

### Glacier
For data archiving, ie, having to save data for 7 years legally.

S3 Glacier
* Costs the same or less than on-premise solutions
* Retrieval times configurable from minutes to hours (first byte latency)

S3 Glacier Deep Archive
* Lowest cost of all storage classes.
* Retrieval time is 12 hours (so you won't get the data for half a day after submitting a request, which is its first byte latency)

# Bucket Permissions and Management
## Bucket Permissions
### Block Public Access
Lets you choose whether the bucket is blocked to all public access, which is the default, or open it up to various levels of public access. 

### Access Control List (ACL)
Allow you to set detailed permissions all the way down to individual objects. If there's a particular object in a particular bucket, you can block down to that object, ie a spreadsheet, that no one is allowed to view it except for Benedict Cumberbatch, who works in the entertainment department. 

Can use a bucket ACL or a bucket policy to control permissions.

### CORS configuration
Pending.

## Management
### LifeCycle
Pending
### Replication
Pending

# Make a Bucket in S3 Using the Terminal
Make sure you have access to your EC2 instance via the terminal (for setting up an EC2 instance, [see my EC2.md file](https://github.com/SharinaS/Cloud-Engineering-Fundamentals/blob/master/EC2.md) - you'll be in a directory that looks like:  

```
[root@ip-111-11-11-11 ec2-user]#
```
### Make a bucket
The format is **aws console + the service + action + name of new thing**.

*Note that the name of the bucket must be lowercase.*
```
aws s3 mb s3://supercooldemobucket
```

### List all the buckets you have in S3:
```
aws s3 ls
```

### Upload Content
To upload content into the bucket from the current location:
(for example, you can make a demo txt file with some content in it using `echo "this is some cool content" > cool-file-name.txt`

... use `ls` to prove to yourself that the file now exists within the current directory)

```
aws s3 cp demoCCP_textfile.txt s3://supercooldemobucket
```
![screenshot of bucket in amazon console](/assets/demoTextFile.png)

# Resources
* A Cloud Guru's [AWS Certified Cloud Practitioner](https://acloud.guru/learn/aws-certified-cloud-practitioner) Coursec
* [S3 FAQs](https://aws.amazon.com/s3/faqs/)
