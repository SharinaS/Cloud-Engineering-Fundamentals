# About S3
S3 is one of the oldest services in AWS. 

Simple Storage Service 
* Static files
* Not suitable for installing an OS on
* Not used for hosting a DB

S3 is a safe place to store files - object files / static files

Upon upload, an HTTP 200 code will come back to your browser is the upload is successful. 


### Object-Based Storage 
Object is made up of:
* Key - name of the object
* Value - the data and is made up of a sequence of bytes
* Version ID - important for versioning
  * S3 allows for versioning!
* Metadata - data about data you're storing
* Subresources
  * Access control lists - the permissions of that object. You can lock each object down individually (versus bucket level)
  * Torrent

### File Size
File size can be from 0 Bytes to 5 TB. 

### Universal Namespace
A universal namespace, so the names must be unique globally. This is because it is creating a web address. It will have the bucket name, region, amazonaws.com. It will create a DNS web name, in other words.

### Data Consistency
S3 has Read after Write consistency for PUTS of new Objects, which means as soon as you upload the object, you're able to read it.

Eventual consistency for overwrite PUTS and DELETES (can take some time to propogates), meaning it will take a bit to update or delete a file. Before the change propogates, you'll be reading the earlier version. 

#### Amazon Guarantees
Built for 99.99% availability, and guaranteed at 99.99% availability.

99.999999999999 (eleven 9's) guarantees durability, so if an object is put into S3, it will likely not be lost, in other words. 

### Lifecycle Management
You can specify that when an object is x days old, move that object to another bucket or glacier.

### Versions
You can have multiple versions within an S3 bucket

### Encryption
You can encyrpt the object at rest

### MFA Delete
You can turn on multi-factor authentication to delete an object, which means the object is protected so it isn't accidentally deleted.

### Access Control Lists & Bucket Policies
Allows for securing of data

## S3 Transfer Acceleration
File transfers occur very fast and securely.

CloudFront's Edge Locations are used to make this happen. When a user uploads a file, it uploads to an edge location, versus to the S3 bucket. Then, this goes over the Amazon backbone network, taking the contents directly back to where the bucket is located. This speeds up user's upload times. 


## Cross Region Replication
If you have a bucket in region 1, and you want to replicate the bucket in region 2 (for disaster recovery and high availability). Upon upload in region 1, and you have cross region replication turned on, the bucket in region 2 will appear. 

## Fees
* Storage - the more you store, the more you're billed - you pay by the gig
  * If you have a lot of objects, it incurs "Monitoring and Automation," which is per 1000 objects.
* Requests - more requests, the more the money
* Storage Management Pricing, so the tier you choose will determine the price
* Data Transfer Pricing
* Transfer Acceleration
* Cross Region Replication

# Tiered Storage
When in doubt, choose Intelligent Tiering.

## Storage Classes
All the following have a first byte latency (how quickly you can access your data) of milliseconds.

S3 Standard
* 99.99% availability and 99.99999999999% durability
* stored redundently in multiple facilities
* Can handle the loss of 2 facilities at the same time
* Most expensive, so it's good to avoid, unless you've got 1000s or millions of objects. Consider using Intelligent Tiering instead. 

S3 - IA
* IA = infrequently accessed 
* For data that does not need to be accessed all that often
* Less expensive than S3 Standard.

S3 One Zone - IA
* Storage in only one availability zone
* Infrequently accessed
* Takes the place of RRS, a service that is being phased out. RRS is like S3 One Zone, when you want something cheap and you don't care about accessing the data much.
* A good option if you don't want to use Intelligent Tiering / Standard.
* Good to choose if you don't care about redundancy, and want something cheaper than S3 Standard, and still want to frequently access the object.

S3 - Intellgient Tiering
* AI will analyze how often an object is accessed, and will automatically move data to the most cost-effective tier.
* No performance impact or operational overhead.
* A good tier to make use of.

## Glacier
For data archiving, ie, having to save data for 7 years legally. 

## Types of Glacier
### S3 Glacier
* Costs the same or less than on-premise solutions
* Retrieval times configurable from minutes to hours (first byte latency)

![screenshot of putting an object into Glacier](/assets/glacier.png)

### S3 Glacier Deep Archive
* Lowest cost of all storage classes.
* Retrieval time is 12 hours (so you won't get the data for half a day after submitting a request, which is its first byte latency)

## Restoring An Object From Glacier
Note that restoring an object from Glacier costs money. The least expensive way of restoring an object is with bulk retrieval. You'll have to note how many days you want the object restored for.

![screenshot of restoring an object](/assets/restorefromglacier.png)

# Security - Bucket Permissions and Management
By default, the buckets are private.

S3 buckets can be configured to create access logs, which keeps track of requirests made to the bucket. You can have those access logs sent to another bucket in the same or another account for storage. 

## Bucket Permissions
### Block Public Access
Lets you choose whether the bucket is blocked to all public access, which is the default, or open it up to various levels of public access. 

### Access Control List (ACL)
Allow you to set detailed permissions all the way down to individual objects. If there's a particular object in a particular bucket, you can block down to that object, ie a spreadsheet, that no one is allowed to view it except for Benedict Cumberbatch, who works in the entertainment department. 

Can use a bucket ACL or a bucket policy to control permissions.

### CORS configuration
Pending.

## Management
### LifeCycle
Pending
### Replication
Pending

## Encryption
Encryption in Transit - HTTPS - achieved with SSL, aka TLS.

If a person breaks into a server, grabs a harddisk drive, and tries to access it, the objects that are encrypted will not be able to be read (theoretically, I guess)... unless they have the AWS key.

### Type of Encryption
Server Side (Amazon)
* S3 Managed Key, aka SSE-S3 (Server side encryption S3)
* AWS Key Management Service, aka Managed Keys SSE-KMS (server side encryption with KMS)
* Server side encryption with customer provided keys - SSE-C

Cient Side 
* You encrypt the object and upload it to S3.

### How to Change Encryption in Console
* Go to the bucket, click on the object's radio button. 
* On the right, a blue titled box will appear. The section called "Properties" has a row called "Encryption."
* You can choose None, AES-256 or AWS-KMS
  * AES-256 is the S3 Managed Key
  * AWS-KMS allows you to use aws/s3 keys, or you can choose to use your own custom keys (custom KMS ARN).

![screenshot of encrypted object](/assets/encryption.png)

# Versioning
Once enabled, you can't disable it, only suspended.

Integrates with lifecycle rules

Uses MFA, which prevents folks from deleting objects. 

## How To Add Versioning
* Create a bucket.
* Change the settings for the bucket so objects in the bucket can be public (this will make experimenting with the objects easier, since you'll be able to see their content).
* Go to properties and click on Versioning. Choose `Enable versioning`. 

![screenshot of enabling version control in S3](/assets/versioningenabled.png)

* Upload your first object, say a txt file with a little content, and make it public.
* Edit your txt file and re-upload. 
* Up above the object listed in the bucket, you'll see an option to `Hide` or `Show` Versions. Toggle it to say `Show`. You should now see two versions of the same object! 

![screenshot of S3 version option](/assets/s3versions.png)

* To view the newest version, you'll have to make it public, FYI. 

## Storage Consideration (= billing consideration)
When you are uploading new versions after editing files, note that each new version can be much larger than the last one. When architecting, keep this in mind, since if you expect to make many changes, you may find it's better to not upload every version, since storage requirements can exponentially increase within the bucket. 

## Deleting After Adding Versioning
To delete a version, the user must have MFA set-up, which provides an additional layer of security against accidental deletion.

To delete a particular version, click on that the radio button next to that version. To do this, make sure that the Versions toggle is set to `Show`.

When deleting the entire object, and you have Versions toggled to `Hide`, it will create a new version of that object that is of status deleted. You can un-delete the object by deleting that version, and it will restore that object to the prior version.

# Lifecycle Management
### About
When you add a lifecycle rule, it will automate transitioning your objects to different tiers of storage. When you specify it, it will take care of expiring the objects too. 

### How To
Go to S3. Go into a bucket that has versioning enabled. Click on the tab called "Management." You can then click on the button called "Lifecycle." 

Click the button "Add lifecycle rule," which will add a rule to the bucket.

![screenshot of lifecycle rule screen](/assets/lifecyclerule.png)

Can add tags when adding a rule name. Tags will make sure the rule only applies to those tags. If you want to apply the rule to the entire bucket, skip using tags.

Next, you specify whether to have the rule apply to current or past versions of the object in the bucket. 
* When you click on "Current version," you need to `+Add transition`. This will let you choose what tier to transition to after a certain number of time. 

![screenshot of lifecycle rules](/assets/lifecyclerules.png)

You can then "Configure expiration" for both current and previous versions. When you click on the radio button for "Clean up incomplete multipart uploads, this refers to uploads that occur in various pieces. Sometimes you can have files that don't upload correctly, so this will help clean those poorly loaded files from lingering. 

![screenshot of lifecycle rules](/assets/lifecycleconfiguration.png)

# Cross Region Replication
* Versioning must be enabled on both the source and destination buckets. 
* Files in existing buckets are not replicated automatically
* All subsequent updated files will be replicated automatically
* Delete markers are not replicated. 
* Deleting indivudal version or delete markers will not be replicated

# S3 Transfer Acceleration
Uses CloudFront Edge Network to speed up S3 uploads. You upload to an S3 edge location, and then that object is transfered over to S3.

You'll get a URL to upload to, which looks something like: "_____.s3-accelerate.amazonaws.com"

There is a tool to test it:
* "Amazon S3 Transfer Acceleration Speed Comparison"
* Shows multiple bar graphs after running it that shows various regions and how fast it is in that region. 

# CloudFront
A Content Delivery Network (CDN) 

Delivers webpages and other webcontent based on the geographical location of the user, content and the server. 

When the user goes to the website, the user will pull the content down directly from where that website's region is. 

## Definitions
### Edge Location
A location where content will be cached. It is sepearate from a region or availability zone.

They are not just read-only - you can write to them too.

Objects are cached for the life of the TTL (Time to Live).

You can clear cached objects, but you will be charged. 

### Origin 
Origin of all the files that the CDN will distribute. Can be route53, an elastic load balancer, ec2 instance, or s3 bucket.

### Distribution
A collection of edge locations

## How it Works
The user tries to access a website. The website is cached to the edge location if it's not already there. The next user can then access the content.

Can deliver the entire website, including dynamic and streaming content. 

Distributions can be either Web Distribution (typically used for websites) or RTMP (for media streaming - Adobe Flash Media Servers).

CloudFont is a global service - you don't choose a region. 

### To set it up, you "Create a Distribution"

You have the option of choosing "Restrict Bucket Access" - can be useful when using signed URLs. 

Generally can set everything as default. Note that TTL is time to live, as in how long the object will be cached for. TTL is in seconds.

You can restrict access using signed URLs, or signed cookies. Don't have to do this though. 

Can take a while, sometimes up to an hour, to deploy the distribution once the last button is clicked. 

Copy the Domain Name. 

Note that you can "Create Invalidation." This means that the distribution will no longer be in the edge locations. If you push out some data, you figure something is wrong, and in the trouble-shooting steps you can "Create Invalidation."

# Snowball
Can import to S3 and export from S3.

A petabyte-scale data transport solution. It's a way to get a huge amount of data into AWS. Deals with problems like high network costs, security issues, etc. 

Comes in either 50TB or 80TB size. 

Once the data transfer job has been processed, AWS performs a software erasure of the Snowball appliance. 

There is also a Snowball Edge, which is 100TB of storage and computer abilities. This thing gives not just storage, but also compute! It can help support local workloads in remote or offline locations. You can stick one of these things in an aircraft as you're testing the airplane. It's like having a mini AWS available. 

Snowmobile is an Exabyte-scale data transfer service. You can transfer up to 100PB per Snomobile. The thing is pulled by a Semi truck. Can move video libraries, and other huge amounts of data. 

# Storage Gateway
Used to move data into AWS. It's a service that connects an on premise software applicant with cloud based storage. 

It's a virtual or physical device that replicates your data into AWS. 

It's available to download as an AMI that you intall in your datacenter. SG supports either VMware ESXi or Microsoft Hyper-V. 



-------------

# Make a Bucket in S3 Using the Terminal
Make sure you have access to your EC2 instance via the terminal (for setting up an EC2 instance, [see my EC2.md file](https://github.com/SharinaS/Cloud-Engineering-Fundamentals/blob/master/EC2.md) - you'll be in a directory that looks like:  

```
[root@ip-111-11-11-11 ec2-user]#
```
### Make a bucket
The format is **aws console + the service + action + name of new thing**.

*Note that the name of the bucket must be lowercase.*
```
aws s3 mb s3://supercooldemobucket
```

### List all the buckets you have in S3:
```
aws s3 ls
```

### Upload Content
To upload content into the bucket from the current location:
(for example, you can make a demo txt file with some content in it using `echo "this is some cool content" > cool-file-name.txt`

... use `ls` to prove to yourself that the file now exists within the current directory)

```
aws s3 cp demoCCP_textfile.txt s3://supercooldemobucket
```
![screenshot of bucket in amazon console](/assets/demoTextFile.png)

# Resources
* A Cloud Guru's [AWS Certified Cloud Practitioner](https://acloud.guru/learn/aws-certified-cloud-practitioner) Course
* A Cloud Guru's [AWS Certified Solutions Architect Associate](https://acloud.guru) Course
* [S3 FAQs](https://aws.amazon.com/s3/faqs/)
* [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/)
